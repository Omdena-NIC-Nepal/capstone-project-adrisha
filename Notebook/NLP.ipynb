{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ddd5d56d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "import re\n",
    "from googletrans import Translator\n",
    "import spacy\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "9e301df2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 15:23:46,120 - INFO - Starting article processing...\n",
      "2025-04-28 15:23:46,121 - INFO - Scraping https://www.facebook.com/okhaldhungakhabar/photos/ओखलढुंगामा-भारी-वर्षा-ओखलढुंगामा-पछिल्लो-२४-घण्टामा-भारी-वर्षा-भएको-छ-जल-तथा-मौस/1230986501622455/?_rdr...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Processing Rainfall-Related Articles ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-28 15:23:47,584 - WARNING - No text extracted from https://www.facebook.com/okhaldhungakhabar/photos/ओखलढुंगामा-भारी-वर्षा-ओखलढुंगामा-पछिल्लो-२४-घण्टामा-भारी-वर्षा-भएको-छ-जल-तथा-मौस/1230986501622455/?_rdr\n",
      "2025-04-28 15:23:47,585 - INFO - Scraping https://janaprashasan.com/2023/08/25/41455/...\n",
      "2025-04-28 15:23:50,878 - INFO - Scraping https://emountaintv.com/ne/255134/...\n",
      "2025-04-28 15:23:53,321 - INFO - Scraping https://khabarhub.com/2024/28/693305/...\n",
      "2025-04-28 15:23:54,954 - INFO - Scraping https://www.makalukhabar.com/2019/07/971905/...\n",
      "2025-04-28 15:23:56,036 - INFO - Scraping https://ekantipur.com/madhesh-pradesh/2019/07/14/156309591531712873.html...\n",
      "2025-04-28 15:24:01,859 - INFO - Scraping https://www.enepalese.com/2019/07/238090.html...\n",
      "2025-04-28 15:24:02,660 - INFO - Scraping https://kathmandupress.com/detail/48024...\n",
      "2025-04-28 15:24:03,647 - WARNING - No text extracted from https://kathmandupress.com/detail/48024\n",
      "2025-04-28 15:24:03,650 - INFO - Scraping https://maitrinews.com/2020/07/20/55953...\n",
      "2025-04-28 15:24:06,065 - ERROR - Error translating text: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "2025-04-28 15:24:06,081 - INFO - Scraping https://www.prasashan.com/2024/10/01/599004/...\n",
      "2025-04-28 15:24:09,296 - WARNING - No text extracted from https://www.prasashan.com/2024/10/01/599004/\n",
      "2025-04-28 15:24:09,298 - INFO - Scraping https://hamrosanchar.com/archives/180678...\n",
      "2025-04-28 15:24:11,938 - INFO - Scraping https://nepalkhabar.com/society/133560-2022-6-10-7-49-21...\n",
      "2025-04-28 15:24:14,873 - INFO - Scraping https://www.facebook.com/brok.curse0911/...\n",
      "2025-04-28 15:24:18,285 - WARNING - No text extracted from https://www.facebook.com/brok.curse0911/\n",
      "2025-04-28 15:24:18,286 - INFO - Scraping https://sangalokhabar.com/113458...\n",
      "2025-04-28 15:24:21,314 - INFO - Scraping https://nepalstatus.com/news/2706963715...\n",
      "2025-04-28 15:24:22,111 - WARNING - No text extracted from https://nepalstatus.com/news/2706963715\n",
      "2025-04-28 15:24:22,113 - INFO - Scraping https://nepallive.com/story/222923...\n",
      "2025-04-28 15:24:23,818 - WARNING - No text extracted from https://nepallive.com/story/222923\n",
      "2025-04-28 15:24:23,822 - INFO - Scraping https://www.setopati.com/social/74197...\n",
      "2025-04-28 15:24:24,292 - WARNING - No text extracted from https://www.setopati.com/social/74197\n",
      "2025-04-28 15:24:24,294 - INFO - Scraping https://nayapage.com/archives/588345...\n",
      "2025-04-28 15:24:27,653 - INFO - Scraping https://www.nayapatrikadaily.com/news-details/73227/2021-10-23...\n",
      "2025-04-28 15:24:30,744 - WARNING - No text extracted from https://www.nayapatrikadaily.com/news-details/73227/2021-10-23\n",
      "2025-04-28 15:24:30,746 - INFO - Scraping https://gorkhapatraonline.com/news/113627...\n",
      "2025-04-28 15:24:32,327 - WARNING - No text extracted from https://gorkhapatraonline.com/news/113627\n",
      "2025-04-28 15:24:32,332 - INFO - Scraping https://hamrakura.com/news-details/129237/video...\n",
      "2025-04-28 15:24:32,966 - WARNING - No text extracted from https://hamrakura.com/news-details/129237/video\n",
      "2025-04-28 15:24:32,968 - INFO - Scraping https://www.facebook.com/mfd.nepal.5...\n",
      "2025-04-28 15:24:34,211 - WARNING - No text extracted from https://www.facebook.com/mfd.nepal.5\n",
      "2025-04-28 15:24:34,212 - INFO - Scraping https://www.onlinekhabar.com/2023/06/1322861/बाढी-पहिरोले-ताप्लेजुङका...\n",
      "2025-04-28 15:24:34,638 - INFO - Scraping https://www.bbc.com/nepali/news/2015/06/150611_taplejung...\n",
      "2025-04-28 15:24:37,535 - INFO - Scraping https://nagariknews.nagariknetwork.com/social-affairs/1209461-1687151534.html...\n",
      "2025-04-28 15:24:41,308 - INFO - Scraping https://baahrakhari.com/detail/391043...\n",
      "2025-04-28 15:24:44,981 - INFO - Scraping https://ekantipur.com/ampnews/2017-07-25/20170725074750.html...\n",
      "2025-04-28 15:24:45,812 - ERROR - Error translating text: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "2025-04-28 15:24:45,831 - INFO - Scraping https://newssanjal.com/content/2022/09/25/24460/...\n",
      "2025-04-28 15:24:49,849 - INFO - Scraping https://kantipurtv.com/news/2024/10/03/1727921265.html...\n",
      "2025-04-28 15:24:50,069 - WARNING - No text extracted from https://kantipurtv.com/news/2024/10/03/1727921265.html\n",
      "2025-04-28 15:24:50,072 - INFO - Scraping https://www.bbc.com/nepali/articles/cd1nz5zn1n1o...\n",
      "2025-04-28 15:24:51,449 - ERROR - Error translating text: the JSON object must be str, bytes or bytearray, not NoneType\n",
      "2025-04-28 15:24:51,461 - INFO - Performing sentiment analysis...\n",
      "2025-04-28 15:24:51,516 - INFO - Performing named entity recognition...\n",
      "2025-04-28 15:24:53,784 - INFO - Performing topic modeling...\n",
      "2025-04-28 15:24:53,793 - INFO - adding document #0 to Dictionary<0 unique tokens: []>\n",
      "2025-04-28 15:24:53,799 - INFO - built Dictionary<631 unique tokens: ['according', 'adopt', 'afternoon', 'alert', 'areas']...> from 18 documents (total 1718 corpus positions)\n",
      "2025-04-28 15:24:53,801 - INFO - Dictionary lifecycle event {'msg': \"built Dictionary<631 unique tokens: ['according', 'adopt', 'afternoon', 'alert', 'areas']...> from 18 documents (total 1718 corpus positions)\", 'datetime': '2025-04-28T15:24:53.801706', 'gensim': '4.3.3', 'python': '3.10.12 (main, Apr 15 2025, 12:28:14) [Clang 14.0.0 (clang-1400.0.29.202)]', 'platform': 'macOS-12.7.6-x86_64-i386-64bit', 'event': 'created'}\n",
      "2025-04-28 15:24:53,814 - INFO - using symmetric alpha at 0.3333333333333333\n",
      "2025-04-28 15:24:53,820 - INFO - using symmetric eta at 0.3333333333333333\n",
      "2025-04-28 15:24:53,825 - INFO - using serial LDA version on this node\n",
      "2025-04-28 15:24:53,858 - INFO - running online (multi-pass) LDA training, 3 topics, 10 passes over the supplied corpus of 18 documents, updating model once every 18 documents, evaluating perplexity every 18 documents, iterating 50x with a convergence threshold of 0.001000\n",
      "2025-04-28 15:24:53,920 - INFO - -7.370 per-word bound, 165.5 perplexity estimate based on a held-out corpus of 18 documents with 1718 words\n",
      "2025-04-28 15:24:53,921 - INFO - PROGRESS: pass 0, at document #18/18\n",
      "2025-04-28 15:24:54,064 - INFO - topic #0 (0.333): 0.017*\"office\" + 0.014*\"water\" + 0.012*\"district\" + 0.012*\"floods\" + 0.012*\"said\" + 0.010*\"siraha\" + 0.010*\"river\" + 0.008*\"र\" + 0.008*\"दशमलव\" + 0.008*\"flood\"\n",
      "2025-04-28 15:24:54,066 - INFO - topic #1 (0.333): 0.146*\"छ\" + 0.087*\"र\" + 0.037*\"कम\" + 0.020*\"सय\" + 0.019*\"दशमलव\" + 0.018*\"तह\" + 0.015*\"एक\" + 0.013*\"तर\" + 0.008*\"सतह\" + 0.008*\"घर\"\n",
      "2025-04-28 15:24:54,069 - INFO - topic #2 (0.333): 0.019*\"र\" + 0.015*\"district\" + 0.014*\"said\" + 0.013*\"दशमलव\" + 0.012*\"airport\" + 0.011*\"सय\" + 0.009*\"landslide\" + 0.009*\"rescue\" + 0.009*\"flood\" + 0.009*\"due\"\n",
      "2025-04-28 15:24:54,071 - INFO - topic diff=1.074772, rho=1.000000\n",
      "2025-04-28 15:24:54,116 - INFO - -5.881 per-word bound, 58.9 perplexity estimate based on a held-out corpus of 18 documents with 1718 words\n",
      "2025-04-28 15:24:54,118 - INFO - PROGRESS: pass 1, at document #18/18\n",
      "2025-04-28 15:24:54,171 - INFO - topic #0 (0.333): 0.023*\"office\" + 0.017*\"water\" + 0.017*\"siraha\" + 0.014*\"district\" + 0.013*\"floods\" + 0.012*\"river\" + 0.009*\"said\" + 0.009*\"police\" + 0.009*\"rain\" + 0.008*\"damaged\"\n",
      "2025-04-28 15:24:54,172 - INFO - topic #1 (0.333): 0.162*\"छ\" + 0.104*\"र\" + 0.040*\"कम\" + 0.029*\"दशमलव\" + 0.029*\"सय\" + 0.020*\"तह\" + 0.018*\"तर\" + 0.017*\"एक\" + 0.011*\"गत\" + 0.009*\"सतह\"\n",
      "2025-04-28 15:24:54,179 - INFO - topic #2 (0.333): 0.017*\"said\" + 0.016*\"district\" + 0.013*\"airport\" + 0.012*\"landslide\" + 0.011*\"taplejung\" + 0.011*\"rescue\" + 0.011*\"due\" + 0.009*\"flood\" + 0.008*\"rain\" + 0.008*\"biratnagar\"\n",
      "2025-04-28 15:24:54,184 - INFO - topic diff=0.544040, rho=0.577350\n",
      "2025-04-28 15:24:54,252 - INFO - -5.634 per-word bound, 49.7 perplexity estimate based on a held-out corpus of 18 documents with 1718 words\n",
      "2025-04-28 15:24:54,253 - INFO - PROGRESS: pass 2, at document #18/18\n",
      "2025-04-28 15:24:54,282 - INFO - topic #0 (0.333): 0.025*\"office\" + 0.019*\"siraha\" + 0.018*\"water\" + 0.015*\"district\" + 0.014*\"floods\" + 0.012*\"river\" + 0.010*\"police\" + 0.010*\"rain\" + 0.008*\"damaged\" + 0.008*\"locals\"\n",
      "2025-04-28 15:24:54,283 - INFO - topic #1 (0.333): 0.168*\"छ\" + 0.111*\"र\" + 0.042*\"कम\" + 0.033*\"दशमलव\" + 0.032*\"सय\" + 0.021*\"तह\" + 0.019*\"तर\" + 0.018*\"एक\" + 0.012*\"गत\" + 0.009*\"सतह\"\n",
      "2025-04-28 15:24:54,286 - INFO - topic #2 (0.333): 0.019*\"said\" + 0.017*\"district\" + 0.014*\"airport\" + 0.013*\"landslide\" + 0.012*\"taplejung\" + 0.011*\"rescue\" + 0.011*\"due\" + 0.009*\"biratnagar\" + 0.009*\"flood\" + 0.009*\"rain\"\n",
      "2025-04-28 15:24:54,289 - INFO - topic diff=0.269437, rho=0.500000\n",
      "2025-04-28 15:24:54,322 - INFO - -5.576 per-word bound, 47.7 perplexity estimate based on a held-out corpus of 18 documents with 1718 words\n",
      "2025-04-28 15:24:54,329 - INFO - PROGRESS: pass 3, at document #18/18\n",
      "2025-04-28 15:24:54,368 - INFO - topic #0 (0.333): 0.026*\"office\" + 0.020*\"siraha\" + 0.018*\"water\" + 0.015*\"district\" + 0.014*\"floods\" + 0.013*\"river\" + 0.010*\"police\" + 0.010*\"rain\" + 0.009*\"damaged\" + 0.008*\"locals\"\n",
      "2025-04-28 15:24:54,370 - INFO - topic #1 (0.333): 0.171*\"छ\" + 0.114*\"र\" + 0.042*\"कम\" + 0.034*\"दशमलव\" + 0.034*\"सय\" + 0.021*\"तह\" + 0.020*\"तर\" + 0.019*\"एक\" + 0.013*\"गत\" + 0.009*\"सतह\"\n",
      "2025-04-28 15:24:54,383 - INFO - topic #2 (0.333): 0.019*\"said\" + 0.017*\"district\" + 0.014*\"airport\" + 0.014*\"landslide\" + 0.013*\"taplejung\" + 0.011*\"rescue\" + 0.011*\"due\" + 0.010*\"biratnagar\" + 0.009*\"flood\" + 0.009*\"rain\"\n",
      "2025-04-28 15:24:54,386 - INFO - topic diff=0.150147, rho=0.447214\n",
      "2025-04-28 15:24:54,422 - INFO - -5.557 per-word bound, 47.1 perplexity estimate based on a held-out corpus of 18 documents with 1718 words\n",
      "2025-04-28 15:24:54,423 - INFO - PROGRESS: pass 4, at document #18/18\n",
      "2025-04-28 15:24:54,459 - INFO - topic #0 (0.333): 0.026*\"office\" + 0.021*\"siraha\" + 0.018*\"water\" + 0.016*\"district\" + 0.014*\"floods\" + 0.013*\"river\" + 0.010*\"police\" + 0.010*\"rain\" + 0.009*\"damaged\" + 0.009*\"locals\"\n",
      "2025-04-28 15:24:54,464 - INFO - topic #1 (0.333): 0.173*\"छ\" + 0.116*\"र\" + 0.043*\"कम\" + 0.035*\"दशमलव\" + 0.034*\"सय\" + 0.021*\"तह\" + 0.021*\"तर\" + 0.019*\"एक\" + 0.013*\"गत\" + 0.009*\"सतह\"\n",
      "2025-04-28 15:24:54,468 - INFO - topic #2 (0.333): 0.020*\"said\" + 0.017*\"district\" + 0.014*\"airport\" + 0.014*\"landslide\" + 0.013*\"taplejung\" + 0.011*\"rescue\" + 0.011*\"due\" + 0.010*\"biratnagar\" + 0.009*\"flood\" + 0.009*\"rain\"\n",
      "2025-04-28 15:24:54,471 - INFO - topic diff=0.087786, rho=0.408248\n",
      "2025-04-28 15:24:54,575 - INFO - -5.549 per-word bound, 46.8 perplexity estimate based on a held-out corpus of 18 documents with 1718 words\n",
      "2025-04-28 15:24:54,577 - INFO - PROGRESS: pass 5, at document #18/18\n",
      "2025-04-28 15:24:54,599 - INFO - topic #0 (0.333): 0.026*\"office\" + 0.021*\"siraha\" + 0.018*\"water\" + 0.016*\"district\" + 0.014*\"floods\" + 0.013*\"river\" + 0.010*\"rain\" + 0.010*\"police\" + 0.009*\"damaged\" + 0.009*\"locals\"\n",
      "2025-04-28 15:24:54,601 - INFO - topic #1 (0.333): 0.174*\"छ\" + 0.116*\"र\" + 0.043*\"कम\" + 0.036*\"दशमलव\" + 0.035*\"सय\" + 0.021*\"तह\" + 0.021*\"तर\" + 0.019*\"एक\" + 0.013*\"गत\" + 0.009*\"सतह\"\n",
      "2025-04-28 15:24:54,603 - INFO - topic #2 (0.333): 0.020*\"said\" + 0.017*\"district\" + 0.014*\"airport\" + 0.014*\"landslide\" + 0.013*\"taplejung\" + 0.012*\"rescue\" + 0.011*\"due\" + 0.010*\"biratnagar\" + 0.009*\"flood\" + 0.009*\"rain\"\n",
      "2025-04-28 15:24:54,605 - INFO - topic diff=0.053209, rho=0.377964\n",
      "2025-04-28 15:24:54,629 - INFO - -5.546 per-word bound, 46.7 perplexity estimate based on a held-out corpus of 18 documents with 1718 words\n",
      "2025-04-28 15:24:54,630 - INFO - PROGRESS: pass 6, at document #18/18\n",
      "2025-04-28 15:24:54,649 - INFO - topic #0 (0.333): 0.027*\"office\" + 0.021*\"siraha\" + 0.018*\"water\" + 0.016*\"district\" + 0.014*\"floods\" + 0.013*\"river\" + 0.010*\"rain\" + 0.010*\"police\" + 0.009*\"damaged\" + 0.009*\"locals\"\n",
      "2025-04-28 15:24:54,650 - INFO - topic #1 (0.333): 0.174*\"छ\" + 0.117*\"र\" + 0.043*\"कम\" + 0.036*\"दशमलव\" + 0.035*\"सय\" + 0.021*\"तह\" + 0.021*\"तर\" + 0.019*\"एक\" + 0.013*\"गत\" + 0.009*\"सतह\"\n",
      "2025-04-28 15:24:54,652 - INFO - topic #2 (0.333): 0.020*\"said\" + 0.017*\"district\" + 0.014*\"airport\" + 0.014*\"landslide\" + 0.013*\"taplejung\" + 0.012*\"rescue\" + 0.012*\"due\" + 0.010*\"biratnagar\" + 0.009*\"flood\" + 0.009*\"rain\"\n",
      "2025-04-28 15:24:54,653 - INFO - topic diff=0.033248, rho=0.353553\n",
      "2025-04-28 15:24:54,685 - INFO - -5.545 per-word bound, 46.7 perplexity estimate based on a held-out corpus of 18 documents with 1718 words\n",
      "2025-04-28 15:24:54,686 - INFO - PROGRESS: pass 7, at document #18/18\n",
      "2025-04-28 15:24:54,708 - INFO - topic #0 (0.333): 0.027*\"office\" + 0.021*\"siraha\" + 0.018*\"water\" + 0.016*\"district\" + 0.014*\"floods\" + 0.013*\"river\" + 0.010*\"rain\" + 0.010*\"police\" + 0.009*\"damaged\" + 0.009*\"locals\"\n",
      "2025-04-28 15:24:54,710 - INFO - topic #1 (0.333): 0.174*\"छ\" + 0.117*\"र\" + 0.043*\"कम\" + 0.036*\"दशमलव\" + 0.035*\"सय\" + 0.021*\"तह\" + 0.021*\"तर\" + 0.019*\"एक\" + 0.013*\"गत\" + 0.009*\"सतह\"\n",
      "2025-04-28 15:24:54,783 - INFO - topic #2 (0.333): 0.020*\"said\" + 0.017*\"district\" + 0.014*\"airport\" + 0.014*\"landslide\" + 0.013*\"taplejung\" + 0.012*\"rescue\" + 0.012*\"due\" + 0.010*\"biratnagar\" + 0.009*\"flood\" + 0.009*\"rain\"\n",
      "2025-04-28 15:24:54,785 - INFO - topic diff=0.021328, rho=0.333333\n",
      "2025-04-28 15:24:54,817 - INFO - -5.544 per-word bound, 46.7 perplexity estimate based on a held-out corpus of 18 documents with 1718 words\n",
      "2025-04-28 15:24:54,820 - INFO - PROGRESS: pass 8, at document #18/18\n",
      "2025-04-28 15:24:54,836 - INFO - topic #0 (0.333): 0.027*\"office\" + 0.021*\"siraha\" + 0.018*\"water\" + 0.016*\"district\" + 0.014*\"floods\" + 0.013*\"river\" + 0.010*\"rain\" + 0.010*\"police\" + 0.009*\"damaged\" + 0.009*\"locals\"\n",
      "2025-04-28 15:24:54,840 - INFO - topic #1 (0.333): 0.175*\"छ\" + 0.117*\"र\" + 0.043*\"कम\" + 0.036*\"दशमलव\" + 0.035*\"सय\" + 0.021*\"तह\" + 0.021*\"तर\" + 0.019*\"एक\" + 0.013*\"गत\" + 0.009*\"सतह\"\n",
      "2025-04-28 15:24:54,854 - INFO - topic #2 (0.333): 0.020*\"said\" + 0.017*\"district\" + 0.014*\"landslide\" + 0.014*\"airport\" + 0.013*\"taplejung\" + 0.012*\"rescue\" + 0.012*\"due\" + 0.010*\"biratnagar\" + 0.009*\"rain\" + 0.009*\"flood\"\n",
      "2025-04-28 15:24:54,858 - INFO - topic diff=0.013999, rho=0.316228\n",
      "2025-04-28 15:24:54,881 - INFO - -5.544 per-word bound, 46.7 perplexity estimate based on a held-out corpus of 18 documents with 1718 words\n",
      "2025-04-28 15:24:54,885 - INFO - PROGRESS: pass 9, at document #18/18\n",
      "2025-04-28 15:24:54,925 - INFO - topic #0 (0.333): 0.027*\"office\" + 0.021*\"siraha\" + 0.019*\"water\" + 0.016*\"district\" + 0.014*\"floods\" + 0.013*\"river\" + 0.010*\"rain\" + 0.010*\"police\" + 0.009*\"damaged\" + 0.009*\"locals\"\n",
      "2025-04-28 15:24:54,937 - INFO - topic #1 (0.333): 0.175*\"छ\" + 0.118*\"र\" + 0.043*\"कम\" + 0.036*\"दशमलव\" + 0.035*\"सय\" + 0.021*\"तह\" + 0.021*\"तर\" + 0.019*\"एक\" + 0.013*\"गत\" + 0.009*\"सतह\"\n",
      "2025-04-28 15:24:54,945 - INFO - topic #2 (0.333): 0.020*\"said\" + 0.017*\"district\" + 0.014*\"landslide\" + 0.014*\"airport\" + 0.013*\"taplejung\" + 0.012*\"rescue\" + 0.012*\"due\" + 0.010*\"biratnagar\" + 0.009*\"rain\" + 0.009*\"flood\"\n",
      "2025-04-28 15:24:54,951 - INFO - topic diff=0.009377, rho=0.301511\n",
      "2025-04-28 15:24:54,959 - INFO - LdaModel lifecycle event {'msg': 'trained LdaModel<num_terms=631, num_topics=3, decay=0.5, chunksize=2000> in 1.10s', 'datetime': '2025-04-28T15:24:54.959545', 'gensim': '4.3.3', 'python': '3.10.12 (main, Apr 15 2025, 12:28:14) [Clang 14.0.0 (clang-1400.0.29.202)]', 'platform': 'macOS-12.7.6-x86_64-i386-64bit', 'event': 'created'}\n",
      "2025-04-28 15:24:54,999 - INFO - Performing text summarization...\n",
      "2025-04-28 15:24:56,041 - INFO - Article processing completed.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLP results saved to ../Rainfall_app/Data/nlp_results.csv\n",
      "\n",
      "Sample NLP Results:\n",
      "          source language  compound  \\\n",
      "0  Janaprashasan       ne    0.4019   \n",
      "1   eMountain TV       ne   -0.2732   \n",
      "2      Khabarhub       en    0.0000   \n",
      "3  Makalu Khabar       ne   -0.9773   \n",
      "4       Kantipur       ne    0.8225   \n",
      "\n",
      "                                           locations  \\\n",
      "0  [Lamjung, Syndunga, Syndung, Sindhunga, Kathma...   \n",
      "1                         [Okhaldhu, Koshi, Gandaki]   \n",
      "2                                                 []   \n",
      "3  [Siraha, Lahan, Siraha, Kiraha, Janakppaldham,...   \n",
      "4  [Siraha, hurricanes, gagan, Kandonha, Water, J...   \n",
      "\n",
      "                                              events        station_mention  \\\n",
      "0                                            [Today]  [Okhaldhunga, Siraha]   \n",
      "1  [today, the previous year, the previous year, ...                     []   \n",
      "2                                                 []                     []   \n",
      "3  [July 27, the last five days, the last five da...               [Siraha]   \n",
      "4  [the long day, the season, a few days, thursda...               [Siraha]   \n",
      "\n",
      "   dominant_topic                                            summary  \n",
      "0               1  Kathmandu: Lamjung, Okhaldhuuu, Syndunga, Synd...  \n",
      "1               0  Weather forecasting Cryastra, which is also ch...  \n",
      "2               1   १२ आश्विन २०८१, शनिबार पढ्न लाग्ने समय :< 1मिनेट  \n",
      "3               0  July 27, Siraha.Market area, including distric...  \n",
      "4               0  FACEBOOK MESSEENGER WHATSAPPPPPER VIBER TWITER...  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Professional NLP Pipeline for Rainfall-Related News Analysis in Eastern Nepal.\n",
    "Scrapes articles from provided URLs, processes Nepali and English text, and performs\n",
    "sentiment analysis, named entity recognition, topic modeling, and text summarization.\n",
    "Prioritizes 18 rainfall stations for relevance to rainfall analysis.\n",
    "\"\"\"\n",
    "\n",
    "import pandas as pd\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import re\n",
    "import nltk\n",
    "from nltk.sentiment.vader import SentimentIntensityAnalyzer\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "from googletrans import Translator\n",
    "import spacy\n",
    "from gensim import corpora\n",
    "from gensim.models import LdaModel\n",
    "from sumy.parsers.plaintext import PlaintextParser\n",
    "from sumy.nlp.tokenizers import Tokenizer\n",
    "from sumy.summarizers.lsa import LsaSummarizer\n",
    "import logging\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Configure logging\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "\n",
    "# Define file paths\n",
    "PREPROCESSED_PATH = '../Data/Preprocessed'\n",
    "OUTPUT_PATH = '../Rainfall_app/Data'\n",
    "\n",
    "# Ensure output directory exists\n",
    "os.makedirs(OUTPUT_PATH, exist_ok=True)\n",
    "\n",
    "# Download NLTK resources\n",
    "nltk.download('vader_lexicon', quiet=True)\n",
    "nltk.download('punkt', quiet=True)\n",
    "nltk.download('stopwords', quiet=True)\n",
    "\n",
    "# Define 18 rainfall stations\n",
    "RAINFALL_STATIONS = [\n",
    "    'Okhaldhunga', 'Khotang Bazaar', 'Siraha', 'Rajbiraj', 'Barmajhiya', 'Chainpur (East)',\n",
    "    'Pakhribas', 'Dhankuta', 'Biratnagar Airport', 'Tarhara', 'Dingla', 'Taplejung',\n",
    "    'Ilam Tea Garden', 'Damak', 'Anarmani Birta', 'Chandri Gadhi', 'Phidim (Panchthar)',\n",
    "    'Kanyam Tea Estate', 'Gaida (Kankai)'\n",
    "]\n",
    "\n",
    "class NLPPipeline:\n",
    "    \"\"\"A modular NLP pipeline for processing rainfall-related news articles in Eastern Nepal.\"\"\"\n",
    "    \n",
    "    def __init__(self):\n",
    "        \"\"\"Initialize the pipeline with necessary components.\"\"\"\n",
    "        self.translator = Translator()\n",
    "        self.sia = SentimentIntensityAnalyzer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.nlp = spacy.load('en_core_web_sm', disable=['parser'])\n",
    "        self.stations = RAINFALL_STATIONS\n",
    "        self.logger = logging.getLogger(__name__)\n",
    "\n",
    "    def scrape_article(self, url):\n",
    "        \"\"\"\n",
    "        Scrape full text from a news article or social media post.\n",
    "        \n",
    "        Args:\n",
    "            url (str): URL of the article or post.\n",
    "        \n",
    "        Returns:\n",
    "            str: Extracted text or None if scraping fails.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            headers = {'User-Agent': 'Mozilla/5.0'}\n",
    "            response = requests.get(url, timeout=10, headers=headers)\n",
    "            response.raise_for_status()\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Try specific content selectors\n",
    "            content = (\n",
    "                soup.find('article') or\n",
    "                soup.find('div', class_=re.compile('content|article-body|news-content|post-content')) or\n",
    "                soup.find('div', class_=re.compile('post|fb-post|status'))\n",
    "            )\n",
    "            if content:\n",
    "                paragraphs = content.find_all('p')\n",
    "                text = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])\n",
    "            else:\n",
    "                # Fallback to all paragraphs\n",
    "                paragraphs = soup.find_all('p')\n",
    "                text = ' '.join([p.get_text(strip=True) for p in paragraphs if p.get_text(strip=True)])\n",
    "            \n",
    "            if not text.strip():\n",
    "                self.logger.warning(f\"No text extracted from {url}\")\n",
    "                return None\n",
    "            return text\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error scraping {url}: {e}\")\n",
    "            return None\n",
    "\n",
    "    def translate_text(self, text, src='ne', dest='en'):\n",
    "        \"\"\"\n",
    "        Translate text from source language to destination language.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Text to translate.\n",
    "            src (str): Source language code (default: 'ne' for Nepali).\n",
    "            dest (str): Destination language code (default: 'en' for English).\n",
    "        \n",
    "        Returns:\n",
    "            str: Translated text or original text if translation fails.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return ''\n",
    "        try:\n",
    "            translation = self.translator.translate(text, src=src, dest=dest)\n",
    "            return translation.text if translation.text else text\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error translating text: {e}\")\n",
    "            return text\n",
    "\n",
    "    def preprocess_text(self, text, language='en'):\n",
    "        \"\"\"\n",
    "        Preprocess text for NLP tasks.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "            language (str): Language code (default: 'en').\n",
    "        \n",
    "        Returns:\n",
    "            str: Cleaned text.\n",
    "        \"\"\"\n",
    "        if not isinstance(text, str) or not text.strip():\n",
    "            return ''\n",
    "        text = text.lower()\n",
    "        text = re.sub(r'http\\S+|www\\S+|@\\w+|#\\w+', '', text)\n",
    "        tokens = word_tokenize(text)\n",
    "        tokens = [t for t in tokens if t.isalpha() and t not in self.stop_words]\n",
    "        return ' '.join(tokens)\n",
    "\n",
    "    def sentiment_analysis(self, texts):\n",
    "        \"\"\"\n",
    "        Perform sentiment analysis using VADER.\n",
    "        \n",
    "        Args:\n",
    "            texts (list): List of cleaned texts.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Sentiment scores (neg, neu, pos, compound).\n",
    "        \"\"\"\n",
    "        sentiments = []\n",
    "        for text in texts:\n",
    "            if text:\n",
    "                scores = self.sia.polarity_scores(text)\n",
    "                sentiments.append({\n",
    "                    'neg': scores['neg'],\n",
    "                    'neu': scores['neu'],\n",
    "                    'pos': scores['pos'],\n",
    "                    'compound': scores['compound']\n",
    "                })\n",
    "            else:\n",
    "                sentiments.append({'neg': 0, 'neu': 0, 'pos': 0, 'compound': 0})\n",
    "        return pd.DataFrame(sentiments)\n",
    "\n",
    "    def named_entity_recognition(self, texts):\n",
    "        \"\"\"\n",
    "        Extract locations, events, and station mentions using spaCy.\n",
    "        \n",
    "        Args:\n",
    "            texts (list): List of texts (translated to English).\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Locations, events, and station mentions per text.\n",
    "        \"\"\"\n",
    "        entities = []\n",
    "        for text in texts:\n",
    "            if text:\n",
    "                doc = self.nlp(text)\n",
    "                locations = [ent.text for ent in doc.ents if ent.label_ == 'GPE']\n",
    "                events = [\n",
    "                    ent.text for ent in doc.ents \n",
    "                    if ent.label_ in ['EVENT', 'DATE'] or \n",
    "                    any(keyword in ent.text.lower() for keyword in ['flood', 'landslide', 'rainfall'])\n",
    "                ]\n",
    "                # Check for station mentions\n",
    "                station_mentions = [\n",
    "                    station for station in self.stations \n",
    "                    if re.search(rf'\\b{re.escape(station)}\\b', text, re.IGNORECASE)\n",
    "                ]\n",
    "                entities.append({\n",
    "                    'locations': locations,\n",
    "                    'events': events,\n",
    "                    'station_mention': station_mentions\n",
    "                })\n",
    "            else:\n",
    "                entities.append({\n",
    "                    'locations': [],\n",
    "                    'events': [],\n",
    "                    'station_mention': []\n",
    "                })\n",
    "        return pd.DataFrame(entities)\n",
    "\n",
    "    def topic_modeling(self, texts, num_topics=3):\n",
    "        \"\"\"\n",
    "        Perform topic modeling using LDA.\n",
    "        \n",
    "        Args:\n",
    "            texts (list): List of cleaned texts.\n",
    "            num_topics (int): Number of topics to extract.\n",
    "        \n",
    "        Returns:\n",
    "            tuple: List of topics (keywords) and dominant topics per text.\n",
    "        \"\"\"\n",
    "        tokenized_texts = [word_tokenize(text) for text in texts if text]\n",
    "        if not tokenized_texts:\n",
    "            self.logger.warning(\"No valid texts for topic modeling.\")\n",
    "            return [], [None] * len(texts)\n",
    "        \n",
    "        dictionary = corpora.Dictionary(tokenized_texts)\n",
    "        corpus = [dictionary.doc2bow(text) for text in tokenized_texts]\n",
    "        try:\n",
    "            lda_model = LdaModel(\n",
    "                corpus, num_topics=num_topics, id2word=dictionary, \n",
    "                passes=10, random_state=42, minimum_probability=0.0\n",
    "            )\n",
    "            topics = [\n",
    "                {'topic': i, 'keywords': [word for word, _ in lda_model.show_topic(i, topn=5)]}\n",
    "                for i in range(num_topics)\n",
    "            ]\n",
    "            dominant_topics = []\n",
    "            for text in texts:\n",
    "                if text:\n",
    "                    bow = dictionary.doc2bow(word_tokenize(text))\n",
    "                    topic_dist = lda_model[bow]\n",
    "                    dominant_topic = max(topic_dist, key=lambda x: x[1])[0] if topic_dist else None\n",
    "                    dominant_topics.append(dominant_topic)\n",
    "                else:\n",
    "                    dominant_topics.append(None)\n",
    "            return topics, dominant_topics\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error in topic modeling: {e}\")\n",
    "            return [], [None] * len(texts)\n",
    "\n",
    "    def summarize_text(self, text, sentences_count=3):\n",
    "        \"\"\"\n",
    "        Summarize text using extractive summarization.\n",
    "        \n",
    "        Args:\n",
    "            text (str): Input text.\n",
    "            sentences_count (int): Number of sentences in summary.\n",
    "        \n",
    "        Returns:\n",
    "            str: Summary text.\n",
    "        \"\"\"\n",
    "        if not text:\n",
    "            return ''\n",
    "        try:\n",
    "            parser = PlaintextParser.from_string(text, Tokenizer('english'))\n",
    "            summarizer = LsaSummarizer()\n",
    "            summary = summarizer(parser.document, sentences_count)\n",
    "            return ' '.join([str(sentence) for sentence in summary])\n",
    "        except Exception as e:\n",
    "            self.logger.error(f\"Error summarizing text: {e}\")\n",
    "            return ''\n",
    "\n",
    "    def process_articles(self, articles):\n",
    "        \"\"\"\n",
    "        Process articles through the NLP pipeline.\n",
    "        \n",
    "        Args:\n",
    "            articles (list): List of dicts with url, source, date, language.\n",
    "        \n",
    "        Returns:\n",
    "            pd.DataFrame: Processed data with NLP outputs.\n",
    "        \"\"\"\n",
    "        self.logger.info(\"Starting article processing...\")\n",
    "        article_data = []\n",
    "        for article in articles:\n",
    "            self.logger.info(f\"Scraping {article['url']}...\")\n",
    "            text = self.scrape_article(article['url'])\n",
    "            if text:\n",
    "                language = article['language']\n",
    "                translated_text = self.translate_text(text, src='ne', dest='en') if language == 'ne' else text\n",
    "                article_data.append({\n",
    "                    'text': text,\n",
    "                    'translated_text': translated_text,\n",
    "                    'clean_text': self.preprocess_text(translated_text, language='en'),\n",
    "                    'source': article['source'],\n",
    "                    'date': article['date'],\n",
    "                    'type': 'article',\n",
    "                    'language': article['language']\n",
    "                })\n",
    "        \n",
    "        if not article_data:\n",
    "            self.logger.error(\"No articles collected.\")\n",
    "            return pd.DataFrame()\n",
    "        \n",
    "        df = pd.DataFrame(article_data)\n",
    "        self.logger.info(\"Performing sentiment analysis...\")\n",
    "        sentiment_df = self.sentiment_analysis(df['clean_text'])\n",
    "        df = pd.concat([df, sentiment_df], axis=1)\n",
    "        \n",
    "        self.logger.info(\"Performing named entity recognition...\")\n",
    "        ner_df = self.named_entity_recognition(df['translated_text'])\n",
    "        df = pd.concat([df, ner_df], axis=1)\n",
    "        \n",
    "        self.logger.info(\"Performing topic modeling...\")\n",
    "        topics, dominant_topics = self.topic_modeling(df['clean_text'])\n",
    "        df['dominant_topic'] = dominant_topics\n",
    "        \n",
    "        self.logger.info(\"Performing text summarization...\")\n",
    "        df['summary'] = df['translated_text'].apply(self.summarize_text)\n",
    "        \n",
    "        # Save topics\n",
    "        with open(os.path.join(OUTPUT_PATH, 'lda_topics.txt'), 'w', encoding='utf-8') as f:\n",
    "            for topic in topics:\n",
    "                f.write(f\"Topic {topic['topic']}: {', '.join(topic['keywords'])}\\n\")\n",
    "        \n",
    "        self.logger.info(\"Article processing completed.\")\n",
    "        return df\n",
    "\n",
    "def main():\n",
    "    \"\"\"Execute the NLP pipeline with predefined articles.\"\"\"\n",
    "    pipeline = NLPPipeline()\n",
    "    \n",
    "    # Define articles with provided URLs\n",
    "    articles = [\n",
    "        {'url': 'https://www.facebook.com/okhaldhungakhabar/photos/ओखलढुंगामा-भारी-वर्षा-ओखलढुंगामा-पछिल्लो-२४-घण्टामा-भारी-वर्षा-भएको-छ-जल-तथा-मौस/1230986501622455/?_rdr', 'source': 'Okhaldhunga Khabar', 'date': '2024-10-01', 'language': 'ne'},\n",
    "        {'url': 'https://janaprashasan.com/2023/08/25/41455/', 'source': 'Janaprashasan', 'date': '2023-08-25', 'language': 'ne'},\n",
    "        {'url': 'https://emountaintv.com/ne/255134/', 'source': 'eMountain TV', 'date': '2024-09-30', 'language': 'ne'},\n",
    "        {'url': 'https://khabarhub.com/2024/28/693305/', 'source': 'Khabarhub', 'date': '2024-09-28', 'language': 'en'},\n",
    "        {'url': 'https://www.makalukhabar.com/2019/07/971905/', 'source': 'Makalu Khabar', 'date': '2019-07-12', 'language': 'ne'},\n",
    "        {'url': 'https://ekantipur.com/madhesh-pradesh/2019/07/14/156309591531712873.html', 'source': 'Kantipur', 'date': '2019-07-14', 'language': 'ne'},\n",
    "        {'url': 'https://www.enepalese.com/2019/07/238090.html', 'source': 'eNepalese', 'date': '2019-07-12', 'language': 'en'},\n",
    "        {'url': 'https://kathmandupress.com/detail/48024', 'source': 'Kathmandu Press', 'date': '2024-09-30', 'language': 'ne'},\n",
    "        {'url': 'https://maitrinews.com/2020/07/20/55953', 'source': 'Maitri News', 'date': '2020-07-20', 'language': 'ne'},\n",
    "        {'url': 'https://www.prasashan.com/2024/10/01/599004/', 'source': 'Prasashan', 'date': '2024-10-01', 'language': 'ne'},\n",
    "        {'url': 'https://hamrosanchar.com/archives/180678', 'source': 'Hamro Sanchar', 'date': '2024-09-30', 'language': 'ne'},\n",
    "        {'url': 'https://nepalkhabar.com/society/133560-2022-6-10-7-49-21', 'source': 'Nepal Khabar', 'date': '2022-06-10', 'language': 'ne'},\n",
    "        {'url': 'https://www.facebook.com/brok.curse0911/', 'source': 'Facebook Brok Curse', 'date': '2024-10-01', 'language': 'ne'},\n",
    "        {'url': 'https://sangalokhabar.com/113458', 'source': 'Sangalo Khabar', 'date': '2024-09-30', 'language': 'ne'},\n",
    "        {'url': 'https://nepalstatus.com/news/2706963715', 'source': 'Nepal Status', 'date': '2024-09-30', 'language': 'ne'},\n",
    "        {'url': 'https://nepallive.com/story/222923', 'source': 'Nepal Live', 'date': '2024-09-30', 'language': 'ne'},\n",
    "        {'url': 'https://www.setopati.com/social/74197', 'source': 'Setopati', 'date': '2017-08-13', 'language': 'ne'},\n",
    "        {'url': 'https://nayapage.com/archives/588345', 'source': 'Naya Page', 'date': '2024-09-30', 'language': 'ne'},\n",
    "        {'url': 'https://www.nayapatrikadaily.com/news-details/73227/2021-10-23', 'source': 'Naya Patrika', 'date': '2021-10-23', 'language': 'ne'},\n",
    "        {'url': 'https://gorkhapatraonline.com/news/113627', 'source': 'Gorkhapatra', 'date': '2024-09-30', 'language': 'ne'},\n",
    "        {'url': 'https://hamrakura.com/news-details/129237/video', 'source': 'Hamrakura', 'date': '2022-10-23', 'language': 'ne'},\n",
    "        {'url': 'https://www.facebook.com/mfd.nepal.5', 'source': 'MFD Nepal', 'date': '2024-10-01', 'language': 'ne'},\n",
    "        {'url': 'https://www.onlinekhabar.com/2023/06/1322861/बाढी-पहिरोले-ताप्लेजुङका', 'source': 'Online Khabar', 'date': '2023-06-18', 'language': 'ne'},\n",
    "        {'url': 'https://www.bbc.com/nepali/news/2015/06/150611_taplejung', 'source': 'BBC Nepali', 'date': '2015-06-11', 'language': 'ne'},\n",
    "        {'url': 'https://nagariknews.nagariknetwork.com/social-affairs/1209461-1687151534.html', 'source': 'Nagarik News', 'date': '2023-06-19', 'language': 'ne'},\n",
    "        {'url': 'https://baahrakhari.com/detail/391043', 'source': 'Baahrakhari', 'date': '2024-09-30', 'language': 'ne'},\n",
    "        {'url': 'https://ekantipur.com/ampnews/2017-07-25/20170725074750.html', 'source': 'Kantipur', 'date': '2017-07-25', 'language': 'ne'},\n",
    "        {'url': 'https://newssanjal.com/content/2022/09/25/24460/', 'source': 'News Sanjal', 'date': '2022-09-25', 'language': 'ne'},\n",
    "        {'url': 'https://kantipurtv.com/news/2024/10/03/1727921265.html', 'source': 'Kantipur TV', 'date': '2024-10-03', 'language': 'ne'},\n",
    "        {'url': 'https://www.bbc.com/nepali/articles/cd1nz5zn1n1o', 'source': 'BBC Nepali', 'date': '2024-10-01', 'language': 'ne'}\n",
    "    ]\n",
    "    \n",
    "    # Process articles\n",
    "    print(\"\\n--- Processing Rainfall-Related Articles ---\")\n",
    "    nlp_df = pipeline.process_articles(articles)\n",
    "    \n",
    "    if nlp_df.empty:\n",
    "        print(\"No data to process. Exiting.\")\n",
    "        return\n",
    "    \n",
    "    # Save NLP results\n",
    "    nlp_output_file = os.path.join(OUTPUT_PATH, 'nlp_results.csv')\n",
    "    nlp_df.to_csv(nlp_output_file, index=False, encoding='utf-8')\n",
    "    print(f\"NLP results saved to {nlp_output_file}\")\n",
    "    \n",
    "    # Print sample results\n",
    "    print(\"\\nSample NLP Results:\")\n",
    "    print(nlp_df[['source', 'language', 'compound', 'locations', 'events', 'station_mention', 'dominant_topic', 'summary']].head())\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "143ee1dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67084b76",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71348c50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
