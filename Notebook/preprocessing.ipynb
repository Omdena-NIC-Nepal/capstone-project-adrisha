{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c3db900a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ab2beef8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded successfully.\n",
      "Stations columns: ['S.N.', 'Station Name', 'Index No.', 'Basin Office', 'Types of Station', 'District', 'Lat(deg)', 'Lon(deg)', 'Ele(meter)', 'Unnamed: 9']\n",
      "Rainfall columns: ['gsid', 'index_no', 'station', 'district', 'year', 'month', 'days', 'rainfall_sum', 'Unnamed: 8', 'Unnamed: 9', 'Unnamed: 10']\n",
      "Missing values handled.\n",
      "Duplicates removed. Stations: 18, Rainfall: 269380 rows.\n",
      "Data merged and features engineered.\n",
      "Feature scaling skipped as rainfall data is already in consistent units (mm).\n",
      "Data split: Training (215504 rows), Testing (53876 rows)\n",
      "Preprocessed data saved to ../Data/Preprocessed\n",
      "Data preprocessing completed successfully.\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\"\"\"\n",
    "Preprocess rainfall and station data for rainfall trend analysis in Eastern Nepal.\n",
    "This script loads raw data, handles missing values and duplicates, performs feature engineering,\n",
    "splits data into training and testing sets, and saves preprocessed data.\n",
    "\"\"\"\n",
    "\n",
    "\n",
    "# Define file paths\n",
    "RAW_DATA_PATH = '../Data/Raw'\n",
    "PREPROCESSED_PATH = '../Data/Preprocessed'\n",
    "\n",
    "# Ensure preprocessed directory exists\n",
    "os.makedirs(PREPROCESSED_PATH, exist_ok=True)\n",
    "\n",
    "def load_data():\n",
    "    \"\"\"Load station and rainfall data from CSV files.\"\"\"\n",
    "    try:\n",
    "        stations = pd.read_csv(os.path.join(RAW_DATA_PATH, 'Eastern Data.csv'))\n",
    "        rainfall = pd.read_csv(os.path.join(RAW_DATA_PATH, 'rainfall_data.csv'))\n",
    "        print(\"Data loaded successfully.\")\n",
    "        print(\"Stations columns:\", stations.columns.tolist())\n",
    "        print(\"Rainfall columns:\", rainfall.columns.tolist())\n",
    "        return stations, rainfall\n",
    "    except FileNotFoundError as e:\n",
    "        raise FileNotFoundError(f\"Error: {e}. Check if the files exist in {RAW_DATA_PATH}\")\n",
    "\n",
    "def handle_missing_values(stations, rainfall):\n",
    "    \"\"\"Handle missing values in the datasets.\"\"\"\n",
    "    # Rainfall: Fill missing 'rainfall_sum' with 0 (assuming no rain)\n",
    "    if 'rainfall_sum' in rainfall.columns:\n",
    "        rainfall['rainfall_sum'] = rainfall['rainfall_sum'].fillna(0)\n",
    "    else:\n",
    "        raise ValueError(\"Column 'rainfall_sum' not found in rainfall data\")\n",
    "    \n",
    "    # Stations: Drop rows with missing critical fields (e.g., Index No.)\n",
    "    if 'Index No.' in stations.columns:\n",
    "        stations = stations.dropna(subset=['Index No.'])\n",
    "    else:\n",
    "        raise ValueError(\"Column 'Index No.' not found in stations data\")\n",
    "    \n",
    "    print(\"Missing values handled.\")\n",
    "    return stations, rainfall\n",
    "\n",
    "def handle_duplicates(stations, rainfall):\n",
    "    \"\"\"Remove duplicate rows from the datasets.\"\"\"\n",
    "    stations = stations.drop_duplicates()\n",
    "    rainfall = rainfall.drop_duplicates()\n",
    "    print(f\"Duplicates removed. Stations: {len(stations)}, Rainfall: {len(rainfall)} rows.\")\n",
    "    return stations, rainfall\n",
    "\n",
    "def preprocess_and_merge(stations, rainfall):\n",
    "    \"\"\"Standardize column names, merge datasets, and create date features.\"\"\"\n",
    "    # Standardize column names\n",
    "    stations.columns = stations.columns.str.strip().str.lower().str.replace(' ', '_').str.replace('.', '_')\n",
    "    rainfall.columns = rainfall.columns.str.strip().str.lower().str.replace(' ', '_')\n",
    "    \n",
    "    # Rename columns for consistency\n",
    "    stations = stations.rename(columns={\n",
    "        'index_no_': 'station_id',\n",
    "        'ele': 'elevation',\n",
    "        'lat': 'latitude',\n",
    "        'lon': 'longitude'\n",
    "    })\n",
    "    rainfall = rainfall.rename(columns={\n",
    "        'index_no': 'station_id',\n",
    "        'station': 'station_name'\n",
    "    })\n",
    "    \n",
    "    # Merge datasets on station_id\n",
    "    merged_data = pd.merge(rainfall, stations, on='station_id', how='left')\n",
    "    \n",
    "    # Create date column\n",
    "    merged_data['date'] = pd.to_datetime(merged_data[['year', 'month', 'days']], errors='coerce')\n",
    "    \n",
    "    # Drop rows with invalid dates\n",
    "    merged_data = merged_data.dropna(subset=['date'])\n",
    "    \n",
    "    # Extract additional time-based features\n",
    "    merged_data['month'] = merged_data['date'].dt.month\n",
    "    merged_data['year'] = merged_data['date'].dt.year\n",
    "    \n",
    "    print(\"Data merged and features engineered.\")\n",
    "    return merged_data\n",
    "\n",
    "def split_data(merged_data):\n",
    "    \"\"\"Split data into training and testing sets without shuffling for time series.\"\"\"\n",
    "    merged_data = merged_data.sort_values('date')\n",
    "    train_data, test_data = train_test_split(merged_data, test_size=0.2, shuffle=False)\n",
    "    print(f\"Data split: Training ({len(train_data)} rows), Testing ({len(test_data)} rows)\")\n",
    "    return train_data, test_data\n",
    "\n",
    "def save_data(train_data, test_data):\n",
    "    \"\"\"Save preprocessed training and testing data.\"\"\"\n",
    "    train_data.to_csv(os.path.join(PREPROCESSED_PATH, 'train_data.csv'), index=False)\n",
    "    test_data.to_csv(os.path.join(PREPROCESSED_PATH, 'test_data.csv'), index=False)\n",
    "    print(f\"Preprocessed data saved to {PREPROCESSED_PATH}\")\n",
    "\n",
    "def main():\n",
    "    \"\"\"Main function to execute preprocessing steps.\"\"\"\n",
    "    stations, rainfall = load_data()\n",
    "    stations, rainfall = handle_missing_values(stations, rainfall)\n",
    "    stations, rainfall = handle_duplicates(stations, rainfall)\n",
    "    merged_data = preprocess_and_merge(stations, rainfall)\n",
    "    print(\"Feature scaling skipped as rainfall data is already in consistent units (mm).\")\n",
    "    train_data, test_data = split_data(merged_data)\n",
    "    save_data(train_data, test_data)\n",
    "    print(\"Data preprocessing completed successfully.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7564638",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "3.10.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
